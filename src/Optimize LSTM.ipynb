{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-22 11:10:52,911] A new study created in memory with name: no-name-45ce6334-076a-44db-8765-4daeac8cb862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n",
      "epoch:   1 last item loss: 0.30796739\n",
      "epoch:   1 train avg. loss: 0.29547159\n",
      "epoch:   1 test avg. loss: 0.28887420\n",
      "Running epoch: 2\n",
      "Running epoch: 3\n",
      "Running epoch: 4\n",
      "Running epoch: 5\n",
      "Running epoch: 6\n",
      "epoch:   6 last item loss: 0.30796739\n",
      "epoch:   6 train avg. loss: 0.28878262\n",
      "epoch:   6 test avg. loss: 0.28887420\n",
      "Running epoch: 7\n",
      "Running epoch: 8\n",
      "Running epoch: 9\n",
      "Total Model Training Runtime: 151.00000000 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-22 13:41:54,811] Trial 0 finished with value: 0.2895258411326466 and parameters: {'lr': 0.04649952469467882}. Best is trial 0 with value: 0.2895258411326466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n",
      "epoch:   1 last item loss: 0.30796739\n",
      "epoch:   1 train avg. loss: 0.28878262\n",
      "epoch:   1 test avg. loss: 0.28887420\n",
      "Running epoch: 2\n",
      "Running epoch: 3\n",
      "Running epoch: 4\n",
      "Running epoch: 5\n",
      "Running epoch: 6\n",
      "epoch:   6 last item loss: 0.30796739\n",
      "epoch:   6 train avg. loss: 0.28878262\n",
      "epoch:   6 test avg. loss: 0.28887420\n",
      "Running epoch: 7\n",
      "Running epoch: 8\n",
      "Running epoch: 9\n",
      "Total Model Training Runtime: 66.00000000 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-09-22 14:48:14,619] Trial 1 finished with value: 0.28878262200996874 and parameters: {'lr': 0.0019122238090015832}. Best is trial 1 with value: 0.28878262200996874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch: 1\n",
      "epoch:   1 last item loss: 0.30796739\n",
      "epoch:   1 train avg. loss: 0.29112288\n",
      "epoch:   1 test avg. loss: 0.28887420\n",
      "Running epoch: 2\n",
      "Running epoch: 3\n",
      "Running epoch: 4\n",
      "Running epoch: 5\n",
      "Running epoch: 6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Optuna example that optimizes multi-layer perceptrons using PyTorch.\n",
    "In this example, we optimize the validation accuracy of hand-written digit recognition using\n",
    "PyTorch and MNIST. We optimize the neural network architecture as well as the optimizer\n",
    "configuration. As it is too time consuming to use the whole MNIST dataset, we here use a small\n",
    "subset of it.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from full_data_load_ep import *\n",
    "from data_processing import *\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DIR = os.getcwd()\n",
    "EPOCHS = 10\n",
    "LOG_INTERVAL = 10\n",
    "\n",
    "\n",
    "def define_model(trial):\n",
    "    # We optimize the number of layers, hidden untis and dropout ratio in each layer.\n",
    "    lstm_layers = trial.suggest_int(\"lstm_layers\", 2, 4)\n",
    "    lstm_hidden = trial.suggest_int(\"lstm_hidden\", 74, 74 * 3)\n",
    "\n",
    "    return Model(lstm_layers=lstm_layers, hidden_size=lstm_hidden)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=74, hidden_size=74, lstm_layers=3, output_size=1, drop=0.2):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.start = time.time()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, lstm_layers, batch_first=True,\n",
    "#                             dropout=drop\n",
    "                           )\n",
    "        self.linear = nn.Linear(hidden_size, output_size)  \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, seasons):\n",
    "\n",
    "        ht = torch.zeros(self.lstm_layers, 1, self.hidden_size)   # initialize hidden state\n",
    "        ct = torch.zeros(self.lstm_layers, 1, self.hidden_size)  # initialize cell state\n",
    "        predictions = torch.Tensor([]) # to store our predictions for season t+1\n",
    "        \n",
    "        hidden = (ht, ct)\n",
    "        \n",
    "        for idx, season in enumerate(seasons):  # here we want to iterate over the time dimension\n",
    "            lstm_input = torch.FloatTensor(season).view(1,1,len(season)) # LSTM takes 3D tensor\n",
    "            out, hidden = self.lstm(lstm_input, hidden) # LSTM updates hidden state and returns output\n",
    "            pred_t = self.linear(out) # pass LSTM output through a linear activation function\n",
    "            pred_t = self.relu(pred_t) # since performance is non-negative we apply ReLU\n",
    "            \n",
    "            predictions = torch.cat((predictions, pred_t)) # concatenate all the predictions\n",
    "\n",
    "        return predictions\n",
    "\n",
    "def get_player_dataset(target='ppg_y_plus_1'):\n",
    "\n",
    "    df = pd.read_csv('../data/player_season_stats.csv')\n",
    "\n",
    "    X, y,_ = prepare_features(df, target)\n",
    "\n",
    "    players = X.index.droplevel(-1).unique() # get number indices\n",
    "    n_players = players.shape[0] # get number of players\n",
    "    train_idx, test_idx = train_test_split(players, test_size=0.3, random_state=18)\n",
    "\n",
    "    print('--- Padding Player Data ---')\n",
    "\n",
    "    X = pad_data(X.reset_index(), players)\n",
    "    y = pad_data(y.reset_index(), players)\n",
    "\n",
    "    X.set_index(['playerid', 'player', 'season_age'], inplace=True)\n",
    "    y.set_index(['playerid', 'player', 'season_age'], inplace=True)\n",
    "\n",
    "    print('--- Generating Player Data ---')\n",
    "    train_seq, train_target = generate_players(X, y, train_idx)\n",
    "    test_seq, test_target = generate_players(X, y, test_idx)\n",
    "\n",
    "    train_idx_bool = pd.Series(list(X.index.droplevel(-1))).isin(train_idx).values\n",
    "    test_idx_bool = pd.Series(list(X.index.droplevel(-1))).isin(test_idx).values\n",
    "\n",
    "    train_real_values = (X[train_idx_bool] != -1).all(axis=1)\n",
    "    test_real_values = (X[test_idx_bool] != -1).all(axis=1)\n",
    "\n",
    "    return X, y, train_seq, train_target, test_seq, test_target, train_idx, test_idx, train_real_values, test_real_values, train_idx_bool, test_idx_bool\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
    "    print('Optimizer: ', optimizer_name, ' LR: ',lr)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Get the player dataset.\n",
    "    X, y, train_seq, train_target, test_seq, test_target, train_idx, test_idx, train_real_values, test_real_values, train_idx_bool, test_idx_bool = get_player_dataset()\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "\n",
    "    # Training of the model.\n",
    "    for epoch in range(EPOCHS):\n",
    "#         model.train()\n",
    "        for seasons, targets in zip(train_seq, train_target):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mask = torch.tensor([(season > -1).all() for season in seasons]) # create mask for real seasons only      \n",
    "            targets = torch.FloatTensor(targets)[mask]\n",
    "            \n",
    "            predictions = model(seasons)\n",
    "            \n",
    "            loss = loss_fn(predictions[mask].squeeze(1), targets) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Validation of the model.\n",
    "#         model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for seasons, targets in zip(test_seq, test_target):\n",
    "                # Limiting validation data.\n",
    "\n",
    "                mask = torch.tensor([(season > -1).all() for season in seasons]) # create mask for real seasons only      \n",
    "                targets = torch.FloatTensor(targets)[mask]\n",
    "                \n",
    "                predictions = model(seasons)\n",
    "                loss = loss_fn(predictions[mask].squeeze(1), targets)\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch}}/{EPOCHS:>{epoch}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        trial.report(train_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return train_loss\n",
    "\n",
    "def train_model(trial):\n",
    "\n",
    "    # Generate the model.\n",
    "#     model = define_model(trial).to(DEVICE)\n",
    "    model = Model(lstm_layers=4, hidden_size=74*3)\n",
    "\n",
    "    # Generate the optimizers.\n",
    "    lr = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
    "\n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "\n",
    "    # old trainer\n",
    "    trainer = Trainer(train_seq, train_target, test_seq, test_target, model, epochs=EPOCHS, lr=lr) # best run: {'epochs' : 100, 'lr' : 0.01}\n",
    "    trainer.train(trial)\n",
    "\n",
    "    return np.nanmean(trainer.train_loss)\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, train_sequences, train_targets, test_sequences, test_targets, model, epochs=200, lr=0.01, batch_size=1, log_per=10000):\n",
    "        self.start = time.time()\n",
    "        self.epochs = epochs\n",
    "        self.log_per = log_per\n",
    "        self.train_sequences = train_sequences\n",
    "        self.train_targets = train_targets\n",
    "        self.test_sequences = test_sequences\n",
    "        self.test_targets = test_targets\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.model = model\n",
    "        \n",
    "    def train(self, trial):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        train_epoch_loss = []\n",
    "        test_epoch_loss = []\n",
    "        \n",
    "        for ep in range(1, self.epochs):\n",
    "            train_loss = []\n",
    "            test_loss = []\n",
    "            print(f'Running epoch: {ep}')\n",
    "            for seasons, targets in zip(self.train_sequences, self.train_targets): # data is a list returning tuple of X, y\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                        \n",
    "                mask = torch.tensor([(season > -1).all() for season in seasons]) # create mask for real seasons only      \n",
    "                targets = torch.FloatTensor(targets)[mask]\n",
    "                \n",
    "                predictions = self.model(seasons)    \n",
    "                # now here, we want to compute the loss between the predicted values\n",
    "                # for each season and the actual values for each season\n",
    "                # TO-DO: random select grouth truth or predicted value in next timestep\n",
    "                loss = self.loss_fn(predictions[mask].squeeze(1), targets) \n",
    "                loss.backward()\n",
    "                optimizer.step() \n",
    "                train_loss.append(loss.item())\n",
    "            \n",
    "            # validate with test set\n",
    "            with torch.no_grad():\n",
    "                for seasons, targets in zip(self.test_sequences, self.test_targets):\n",
    "                    mask = torch.tensor([(season > -1).all() for season in seasons]) # create mask for real seasons only      \n",
    "                    targets = torch.FloatTensor(targets)[mask]\n",
    "\n",
    "                    predictions = self.model(seasons)    \n",
    "                    # now here, we want to compute the loss between the predicted values\n",
    "                    # for each season and the actual values for each season\n",
    "                    # TO-DO: random select grouth truth or predicted value in next timestep\n",
    "                    loss = self.loss_fn(predictions[mask].squeeze(1), targets) \n",
    "                    test_loss.append(loss.item())\n",
    "            \n",
    "            train_epoch_loss.append(np.nanmean(train_loss))\n",
    "            test_epoch_loss.append(np.nanmean(test_loss))\n",
    "            \n",
    "            if ep%5 == 1:\n",
    "                print(f'epoch: {ep:3} last item loss: {loss.item():10.8f}')\n",
    "                print(f'epoch: {ep:3} train avg. loss: {np.nanmean(train_loss):10.8f}')\n",
    "                print(f'epoch: {ep:3} test avg. loss: {np.nanmean(test_loss):10.8f}')\n",
    "                \n",
    "                \n",
    "                \n",
    "            trial.report(np.nanmean(train_loss), ep)\n",
    "\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        print(f'Total Model Training Runtime: {(time.time() - self.start)//60:10.8f} mins')\n",
    "        self.train_loss = train_epoch_loss\n",
    "        self.test_loss = test_epoch_loss  \n",
    "        \n",
    "        return np.nanmean(self.train_loss)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Get the player dataset.\n",
    "    X, y, train_seq, train_target, test_seq, test_target, train_idx, test_idx, train_real_values, test_real_values, train_idx_bool, test_idx_bool = get_player_dataset()\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(train_model, n_trials=5,n_jobs=1)\n",
    "\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apple",
   "language": "python",
   "name": "apple"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
